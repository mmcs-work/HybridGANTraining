{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batchSize=64, beta1=0.5, classes='bedroom', cuda=True, dataroot='../project-5/flow_gan/data', dataset='mnist', dry_run=False, imageSize=64, lr=0.0002, manualSeed=None, ndf=64, netD='./results/netD_epoch_24.pth', netG='', ngf=64, ngpu=1, niter=25, nz=100, outf='./results', workers=2)\n",
      "Random Seed:  9849\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "[0/25][0/938] Loss_D: 0.0100 Loss_G: 30.1351 D(x): 0.9904 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][1/938] Loss_D: 0.0004 Loss_G: 24.9492 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][2/938] Loss_D: 0.0001 Loss_G: 18.5989 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][3/938] Loss_D: 0.0000 Loss_G: 13.7759 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][4/938] Loss_D: 0.0004 Loss_G: 10.3784 D(x): 1.0000 D(G(z)): 0.0004 / 0.0001\n",
      "[0/25][5/938] Loss_D: 0.0063 Loss_G: 12.6254 D(x): 1.0000 D(G(z)): 0.0061 / 0.0000\n",
      "[0/25][6/938] Loss_D: 0.0003 Loss_G: 13.0712 D(x): 1.0000 D(G(z)): 0.0003 / 0.0000\n",
      "[0/25][7/938] Loss_D: 0.0002 Loss_G: 11.9819 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][8/938] Loss_D: 0.0006 Loss_G: 10.5633 D(x): 0.9998 D(G(z)): 0.0004 / 0.0001\n",
      "[0/25][9/938] Loss_D: 0.0011 Loss_G: 10.5713 D(x): 1.0000 D(G(z)): 0.0011 / 0.0000\n",
      "[0/25][10/938] Loss_D: 0.0008 Loss_G: 11.5389 D(x): 0.9999 D(G(z)): 0.0007 / 0.0000\n",
      "[0/25][11/938] Loss_D: 0.0005 Loss_G: 11.4901 D(x): 0.9997 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][12/938] Loss_D: 0.0012 Loss_G: 11.5086 D(x): 0.9996 D(G(z)): 0.0008 / 0.0000\n",
      "[0/25][13/938] Loss_D: 0.0009 Loss_G: 12.7665 D(x): 0.9999 D(G(z)): 0.0008 / 0.0000\n",
      "[0/25][14/938] Loss_D: 0.0003 Loss_G: 12.8577 D(x): 0.9999 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][15/938] Loss_D: 0.0003 Loss_G: 12.1989 D(x): 0.9999 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][16/938] Loss_D: 0.0007 Loss_G: 12.6882 D(x): 0.9998 D(G(z)): 0.0005 / 0.0000\n",
      "[0/25][17/938] Loss_D: 0.0006 Loss_G: 14.4635 D(x): 0.9999 D(G(z)): 0.0005 / 0.0000\n",
      "[0/25][18/938] Loss_D: 0.0002 Loss_G: 13.6512 D(x): 0.9999 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][19/938] Loss_D: 0.0043 Loss_G: 11.0419 D(x): 0.9963 D(G(z)): 0.0002 / 0.0001\n",
      "[0/25][20/938] Loss_D: 0.0010 Loss_G: 14.6800 D(x): 1.0000 D(G(z)): 0.0010 / 0.0000\n",
      "[0/25][21/938] Loss_D: 0.0000 Loss_G: 15.3888 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][22/938] Loss_D: 0.0000 Loss_G: 13.1512 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][23/938] Loss_D: 0.0004 Loss_G: 12.4871 D(x): 0.9999 D(G(z)): 0.0003 / 0.0000\n",
      "[0/25][24/938] Loss_D: 0.0002 Loss_G: 12.1033 D(x): 1.0000 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][25/938] Loss_D: 0.0007 Loss_G: 14.6448 D(x): 1.0000 D(G(z)): 0.0007 / 0.0000\n",
      "[0/25][26/938] Loss_D: 0.0001 Loss_G: 14.3515 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][27/938] Loss_D: 0.0001 Loss_G: 12.6380 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][28/938] Loss_D: 0.0008 Loss_G: 14.1662 D(x): 0.9998 D(G(z)): 0.0006 / 0.0000\n",
      "[0/25][29/938] Loss_D: 0.0003 Loss_G: 13.4982 D(x): 0.9998 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][30/938] Loss_D: 0.0002 Loss_G: 12.6640 D(x): 1.0000 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][31/938] Loss_D: 0.0006 Loss_G: 13.7292 D(x): 1.0000 D(G(z)): 0.0005 / 0.0000\n",
      "[0/25][32/938] Loss_D: 0.0001 Loss_G: 13.4394 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][33/938] Loss_D: 0.0002 Loss_G: 12.4281 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][34/938] Loss_D: 0.0003 Loss_G: 12.1524 D(x): 0.9999 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][35/938] Loss_D: 0.0003 Loss_G: 12.2954 D(x): 0.9999 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][36/938] Loss_D: 0.0006 Loss_G: 13.9184 D(x): 1.0000 D(G(z)): 0.0005 / 0.0000\n",
      "[0/25][37/938] Loss_D: 0.0002 Loss_G: 13.1681 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][38/938] Loss_D: 0.0009 Loss_G: 14.6380 D(x): 0.9998 D(G(z)): 0.0006 / 0.0000\n",
      "[0/25][39/938] Loss_D: 0.0002 Loss_G: 14.1488 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][40/938] Loss_D: 0.0002 Loss_G: 12.1196 D(x): 0.9999 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][41/938] Loss_D: 0.0006 Loss_G: 11.6240 D(x): 0.9996 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][42/938] Loss_D: 0.0002 Loss_G: 12.0237 D(x): 1.0000 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][43/938] Loss_D: 0.0005 Loss_G: 13.2209 D(x): 0.9999 D(G(z)): 0.0004 / 0.0000\n",
      "[0/25][44/938] Loss_D: 0.0001 Loss_G: 12.9329 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][45/938] Loss_D: 0.0001 Loss_G: 11.9292 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][46/938] Loss_D: 0.0002 Loss_G: 11.8114 D(x): 1.0000 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][47/938] Loss_D: 0.0002 Loss_G: 12.2820 D(x): 1.0000 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][48/938] Loss_D: 0.0002 Loss_G: 12.2437 D(x): 0.9999 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][49/938] Loss_D: 0.0002 Loss_G: 12.0827 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][50/938] Loss_D: 0.0003 Loss_G: 12.4049 D(x): 0.9999 D(G(z)): 0.0002 / 0.0000\n",
      "[0/25][51/938] Loss_D: 0.0003 Loss_G: 12.3834 D(x): 0.9998 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][52/938] Loss_D: 0.0001 Loss_G: 12.0215 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][53/938] Loss_D: 0.0004 Loss_G: 13.7233 D(x): 1.0000 D(G(z)): 0.0004 / 0.0000\n",
      "[0/25][54/938] Loss_D: 0.0001 Loss_G: 13.7280 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][55/938] Loss_D: 0.0001 Loss_G: 12.5563 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][56/938] Loss_D: 0.0001 Loss_G: 11.6899 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "[0/25][58/938] Loss_D: 0.0001 Loss_G: 12.7596 D(x): 1.0000 D(G(z)): 0.0001 / 0.0000\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./dcgan.py\", line 235, in <module>\n",
      "    netD.zero_grad()\n",
      "  File \"/nfs/homedirs/qian/anaconda3/envs/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1340, in zero_grad\n",
      "    p.grad.zero_()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ./dcgan.py --dataset 'mnist' --dataroot '../project-5/flow_gan/data' --cuda --outf ./results --netD './results/netD_epoch_24.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/students/winter-term-2020/project-5/dcgan\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
